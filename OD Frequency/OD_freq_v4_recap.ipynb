{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all of PySpark\n",
    "import pyspark\n",
    "\n",
    "# Import garbage collection module so we don't run out of memory while running the script\n",
    "import gc\n",
    "\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Bring in all necessary tables to run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTFS Trips\n",
    "gtfs_trips = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/trips_2020_spring_recap.csv\")\n",
    "\n",
    "gtfs_trips.createOrReplaceTempView(\"gtfs_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTFS Routes\n",
    "gtfs_routes = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/routes_2020_spring_recap.csv\")\n",
    "\n",
    "gtfs_routes.createOrReplaceTempView(\"gtfs_routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTFS Stop Times\n",
    "gtfs_stop_times_import = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/stop_times_2020_spring_recap.csv\")\n",
    "\n",
    "# Parse the arrival time column into hours, minutes, and seconds\n",
    "split_col = pyspark.sql.functions.split(gtfs_stop_times_import['arrival_time'], ':')\n",
    "gtfs_stop_times_parse = gtfs_stop_times_import.withColumn('hour', split_col.getItem(0)) \\\n",
    "       .withColumn('minute', split_col.getItem(1)) \\\n",
    "       .withColumn('second', split_col.getItem(2))\n",
    "\n",
    "# Save the table with parsing to a SQL table\n",
    "gtfs_stop_times_parse.createOrReplaceTempView(\"gtfs_stop_times_parse\")\n",
    "\n",
    "# Now check that times before 3AM have been converted to service day notation (>24h). If not, adjust them\n",
    "gtfs_stop_times_adjust = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         *\n",
    "        ,CASE \n",
    "         WHEN CAST(hour AS integer) < 3 \n",
    "         THEN CAST(hour AS integer) + 24 \n",
    "         ELSE CAST(hour AS integer) END AS hour_adjust\n",
    "        ,CAST(minute AS integer) AS minute_adjust\n",
    "        ,CAST(second AS integer) AS second_adjust\n",
    "    FROM gtfs_stop_times_parse\n",
    "    \"\"\")\n",
    "\n",
    "gtfs_stop_times_adjust.createOrReplaceTempView(\"gtfs_stop_times_adjust\")\n",
    "\n",
    "# Now convert the parsed values into seconds after midnight\n",
    "gtfs_stop_times = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         *\n",
    "        ,(hour_adjust * 3600) + (minute_adjust * 60) + second_adjust AS arrival_time_sec\n",
    "    FROM gtfs_stop_times_adjust\n",
    "    \"\"\")\n",
    "\n",
    "# Save the final, clean table to a SQL table\n",
    "gtfs_stop_times.createOrReplaceTempView(\"gtfs_stop_times\")\n",
    "\n",
    "# Show a preview that the conversion worked\n",
    "spark.sql(\"SELECT MAX(arrival_time), MIN(arrival_time), MAX(arrival_time_sec), MIN(arrival_time_sec) FROM gtfs_stop_times\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTFS Stops\n",
    "gtfs_stops = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/stops_2020_spring_recap.csv\")\n",
    "\n",
    "gtfs_stops.createOrReplaceTempView(\"gtfs_stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTFS Calendar Attributes\n",
    "gtfs_calendar_attributes = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/calendar_attributes_2020_spring_recap.csv\")\n",
    "\n",
    "gtfs_calendar_attributes.createOrReplaceTempView(\"gtfs_calendar_attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTFS Calendar\n",
    "gtfs_calendar = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/calendar_2020_spring_recap.csv\")\n",
    "\n",
    "gtfs_calendar.createOrReplaceTempView(\"gtfs_calendar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hours to Bands\n",
    "bands_hours = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/bands_hours.csv\")\n",
    "\n",
    "bands_hours.createOrReplaceTempView(\"bands_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Grouping\n",
    "stop_grouping_2020_spring = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/od_freq_calculation/stop_grouping_2020_spring.csv\")\n",
    "\n",
    "stop_grouping_2020_spring.createOrReplaceTempView(\"stop_grouping_2020_spring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Let's try running OD Frequency from the beginning, bringing the original SQL script into PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For assigning services to stops\n",
    "# For each master stop ID, find the routes/variants that serve it in each during during each time period\n",
    "\n",
    "weekday_stop_events = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "         t.route_id\n",
    "        ,t.direction_id\n",
    "        ,t.trip_id\n",
    "        ,t.shape_id\n",
    "        ,st.stop_sequence\n",
    "        ,CASE\n",
    "         WHEN s.parent_station IS NULL THEN st.stop_id\n",
    "         ELSE s.parent_station END AS master_stop_id\n",
    "        ,st.hour\n",
    "        ,st.arrival_time_sec\n",
    "        ,ca.service_description\n",
    "    FROM gtfs_trips t\n",
    "        JOIN gtfs_routes r ON t.route_id = r.route_id\n",
    "        JOIN gtfs_stop_times st ON t.trip_id = st.trip_id\n",
    "        JOIN gtfs_stops s ON st.stop_id = s.stop_id\n",
    "        JOIN gtfs_calendar_attributes ca ON t.service_id = ca.service_id\n",
    "        JOIN gtfs_calendar c ON t.service_id = c.service_id\n",
    "    WHERE\n",
    "        ca.service_description = 'Weekday schedule'\n",
    "        AND TO_DATE(CAST(c.start_date AS STRING), 'yyyyMMdd') <= '2020-03-21'\n",
    "        \"\"\")\n",
    "\n",
    "# Run some previews & checks\n",
    "weekday_stop_events.createOrReplaceTempView(\"weekday_stop_events\")\n",
    "spark.sql(\"SELECT * FROM weekday_stop_events LIMIT 10\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM weekday_stop_events\").show()\n",
    "\n",
    "# Check that ferry worked\n",
    "spark.sql(\"SELECT DISTINCT route_id FROM weekday_stop_events WHERE route_id LIKE 'Boat%' LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same, but for Saturday\n",
    "saturday_stop_events = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "         t.route_id\n",
    "        ,t.direction_id\n",
    "        ,t.trip_id\n",
    "        ,t.shape_id\n",
    "        ,st.stop_sequence\n",
    "        ,CASE\n",
    "         WHEN s.parent_station IS NULL THEN st.stop_id\n",
    "         ELSE s.parent_station END AS master_stop_id\n",
    "        ,st.hour\n",
    "        ,st.arrival_time_sec\n",
    "        ,ca.service_description\n",
    "    FROM gtfs_trips t\n",
    "        JOIN gtfs_routes r ON t.route_id = r.route_id\n",
    "        JOIN gtfs_stop_times st ON t.trip_id = st.trip_id\n",
    "        JOIN gtfs_stops s ON st.stop_id = s.stop_id\n",
    "        JOIN gtfs_calendar_attributes ca ON t.service_id = ca.service_id\n",
    "        JOIN gtfs_calendar c ON t.service_id = c.service_id\n",
    "    WHERE\n",
    "        ca.service_description = 'Saturday schedule'\n",
    "        AND TO_DATE(CAST(c.start_date AS STRING), 'yyyyMMdd') <= '2020-03-21'\n",
    "        \"\"\")\n",
    "\n",
    "# Run some previews & checks\n",
    "saturday_stop_events.createOrReplaceTempView(\"saturday_stop_events\")\n",
    "spark.sql(\"SELECT * FROM saturday_stop_events LIMIT 10\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM saturday_stop_events\").show()\n",
    "\n",
    "# Check that CR worked\n",
    "spark.sql(\"SELECT DISTINCT route_id FROM saturday_stop_events WHERE route_id LIKE 'CR%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same, but for Sunday\n",
    "sunday_stop_events = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "         t.route_id\n",
    "        ,t.direction_id\n",
    "        ,t.trip_id\n",
    "        ,t.shape_id\n",
    "        ,st.stop_sequence\n",
    "        ,CASE\n",
    "         WHEN s.parent_station IS NULL THEN st.stop_id\n",
    "         ELSE s.parent_station END AS master_stop_id\n",
    "        ,st.hour\n",
    "        ,st.arrival_time_sec\n",
    "        ,ca.service_description\n",
    "    FROM gtfs_trips t\n",
    "        JOIN gtfs_routes r ON t.route_id = r.route_id\n",
    "        JOIN gtfs_stop_times st ON t.trip_id = st.trip_id\n",
    "        JOIN gtfs_stops s ON st.stop_id = s.stop_id\n",
    "        JOIN gtfs_calendar_attributes ca ON t.service_id = ca.service_id\n",
    "        JOIN gtfs_calendar c ON t.service_id = c.service_id\n",
    "    WHERE\n",
    "        ca.service_description = 'Sunday schedule'\n",
    "        AND TO_DATE(CAST(c.start_date AS STRING), 'yyyyMMdd') <= '2020-03-21'\n",
    "        \"\"\")\n",
    "\n",
    "# Run some previews & checks\n",
    "sunday_stop_events.createOrReplaceTempView(\"sunday_stop_events\")\n",
    "spark.sql(\"SELECT * FROM sunday_stop_events LIMIT 10\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM sunday_stop_events\").show()\n",
    "\n",
    "# Check that ferry worked\n",
    "spark.sql(\"SELECT DISTINCT route_id FROM sunday_stop_events WHERE route_id LIKE 'Boat%' LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now union all the day types\n",
    "stop_events = spark.sql(\"\"\"\n",
    "    SELECT * FROM weekday_stop_events\n",
    "    UNION\n",
    "    SELECT * FROM saturday_stop_events\n",
    "    UNION\n",
    "    SELECT * FROM sunday_stop_events\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "stop_events.createOrReplaceTempView(\"stop_events\")\n",
    "spark.sql(\"SELECT DISTINCT service_description FROM stop_events\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM stop_events\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we clear out the GTFS dataframes and the individual daytypes, to free memory\n",
    "del gtfs_trips\n",
    "del gtfs_routes\n",
    "del gtfs_stop_times\n",
    "del gtfs_stops\n",
    "del gtfs_calendar_attributes\n",
    "del gtfs_calendar\n",
    "del weekday_stop_events\n",
    "del saturday_stop_events\n",
    "del sunday_stop_events\n",
    "gc.collect()\n",
    "gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split up into hours and rename the schedules to day types\n",
    "all_events_format = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         CASE\n",
    "         WHEN service_description = 'Weekday schedule' THEN 'Weekday'\n",
    "         WHEN service_description = 'Saturday schedule' THEN 'Saturday'\n",
    "         WHEN service_description = 'Sunday schedule' THEN 'Sunday'\n",
    "         END AS day_type\n",
    "        ,route_id\n",
    "        ,direction_id\n",
    "        ,trip_id\n",
    "        ,stop_sequence\n",
    "        ,master_stop_id\n",
    "        ,arrival_time_sec\n",
    "        ,hour\n",
    "    FROM stop_events\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "all_events_format.createOrReplaceTempView(\"all_events_format\")\n",
    "spark.sql(\"SELECT * FROM all_events_format LIMIT 100\").show()\n",
    "del stop_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we join in the band labels so we can aggreggate properly by hour, band, or mbta time period moving forward\n",
    "all_events_bands = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         b.band\n",
    "        ,b.day_type\n",
    "        ,r.route_id\n",
    "        ,r.direction_id\n",
    "        ,r.trip_id\n",
    "        ,r.stop_sequence\n",
    "        ,r.master_stop_id\n",
    "        ,r.arrival_time_sec\n",
    "        ,r.hour\n",
    "    FROM all_events_format r\n",
    "    LEFT JOIN bands_hours b ON\n",
    "         r.day_type = b.day_type\n",
    "         AND r.hour >= b.start_hour\n",
    "         AND r.hour < b.end_hour\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "all_events_bands.createOrReplaceTempView(\"all_events_bands\")\n",
    "spark.sql(\"SELECT * FROM all_events_bands LIMIT 100\").show()\n",
    "del all_events_format\n",
    "\n",
    "# Make sure you didn't lose or duplicate any records in the join\n",
    "spark.sql(\"SELECT COUNT(*) FROM all_events_format\").show() # 1057723\n",
    "spark.sql(\"SELECT COUNT(*) FROM all_events_bands\").show() # 1057723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You currently have all stop events that occur at the master ID (parent) level\n",
    "# You now need to create the basis for linking this to the cluster level\n",
    "# To get here, take the services we just pegged to each master ID and assign it to all cluster centers that encompass that master ID\n",
    "cluster_service = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT \n",
    "         g.buffer_center\n",
    "        ,r.day_type\n",
    "        ,r.band\n",
    "        ,r.hour\n",
    "        ,r.arrival_time_sec\n",
    "        ,r.route_id\n",
    "        ,r.direction_id\n",
    "        ,r.trip_id\n",
    "        ,r.master_stop_id\n",
    "        ,r.stop_sequence\n",
    "    FROM all_events_bands r\n",
    "    LEFT JOIN stop_grouping_2020_spring g ON g.child = r.master_stop_id\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "cluster_service.createOrReplaceTempView(\"cluster_service\")\n",
    "spark.sql(\"SELECT * FROM cluster_service LIMIT 100\").show()\n",
    "del all_events_bands\n",
    "\n",
    "# Make sure every stop event got assigned to a buffer center properly\n",
    "spark.sql(\"SELECT * FROM cluster_service WHERE buffer_center IS NULL\").show()\n",
    "\n",
    "# Preview the data, notice that the stop events get duplicated and assigned to multiple buffer centers\n",
    "# This allows you to be flexible with your OD and get nearby service as well as service at the queried stop itself\n",
    "spark.sql(\"SELECT * FROM cluster_service WHERE day_type = 'Weekday' ORDER BY master_stop_id, day_type, hour, arrival_time_sec, buffer_center LIMIT 10000\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take the relevant columns and save them as the universe of potential origins\n",
    "origins = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         buffer_center\n",
    "        ,day_type\n",
    "        ,band\n",
    "        ,hour\n",
    "        ,arrival_time_sec\n",
    "        ,route_id\n",
    "        ,direction_id\n",
    "        ,trip_id\n",
    "        ,stop_sequence\n",
    "    FROM cluster_service\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "origins.createOrReplaceTempView(\"origins\")\n",
    "spark.sql(\"SELECT * FROM origins LIMIT 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same, saving as the universe of potential destinations\n",
    "destinations = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         buffer_center\n",
    "        ,day_type\n",
    "        ,band\n",
    "        ,hour\n",
    "        ,arrival_time_sec\n",
    "        ,route_id\n",
    "        ,direction_id\n",
    "        ,trip_id\n",
    "        ,stop_sequence\n",
    "    FROM cluster_service\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "destinations.createOrReplaceTempView(\"destinations\")\n",
    "spark.sql(\"SELECT * FROM destinations LIMIT 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take these two tables and join all possible combinations to one another\n",
    "# A combination is valid if:\n",
    "### The origin and the destination row correspond to the same trip, direction, and day type\n",
    "### The origin stop comes before the destination stop on that match\n",
    "# You can also add an elapsed time check here, if you'd like (see original SQL query - v7)\n",
    "od_edges = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         o.buffer_center AS buffer_center_origin\n",
    "         ,o.day_type AS day_type_origin\n",
    "         ,o.band AS band_origin\n",
    "         ,o.hour AS hour_origin\n",
    "         ,o.arrival_time_sec AS arrival_time_origin\n",
    "         ,o.route_id AS route_id_origin\n",
    "         ,o.direction_id AS direction_id_origin\n",
    "         ,o.trip_id AS trip_id_origin\n",
    "         ,o.stop_sequence AS stop_sequence_origin\n",
    "         ,d.buffer_center AS buffer_center_dest\n",
    "         ,d.day_type AS day_type_dest\n",
    "         ,d.band AS band_dest\n",
    "         ,d.hour AS hour_dest\n",
    "         ,d.arrival_time_sec AS arrival_time_dest\n",
    "         ,d.route_id AS route_id_dest\n",
    "         ,d.direction_id AS direction_id_dest\n",
    "         ,d.trip_id AS trip_id_dest\n",
    "         ,d.stop_sequence AS stop_sequence_dest\n",
    "         ,d.arrival_time_sec - o.arrival_time_sec AS elapsed_time\n",
    "    FROM origins o\n",
    "    JOIN destinations d \n",
    "        ON o.day_type = d.day_type \n",
    "        AND o.route_id = d.route_id \n",
    "        AND o.trip_id = d.trip_id \n",
    "        AND o.direction_id = d.direction_id\n",
    "    WHERE o.stop_sequence < d.stop_sequence\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "od_edges.createOrReplaceTempView(\"od_edges\")\n",
    "spark.sql(\"SELECT * FROM od_edges LIMIT 100\").show()\n",
    "del cluster_service\n",
    "del origins\n",
    "del destinations\n",
    "\n",
    "# Test it out\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM od_edges\n",
    "    WHERE buffer_center_origin = 'place-coecl' AND buffer_center_dest = 'place-nuniv'\n",
    "    ORDER BY buffer_center_origin, buffer_center_dest, arrival_time_origin LIMIT 1000\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for each OD in each hour, we want to include only the first occurrence of a trip in the events table\n",
    "# This will prevent multiple stop events within the origin on the same trip (e.g. loops or nearby sequential stops) from getting counted as a headway\n",
    "# Note that if the trip stops twice within the cluster at the turn of the time period, these events will be counted separately in each respective period\n",
    "# Trip ID should be unique on its own, but we include route ID and direction ID just to be safe\n",
    "no_duplicates_intermediate = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "        ,trip_id_origin\n",
    "        ,route_id_origin\n",
    "        ,direction_id_origin\n",
    "        ,MIN(arrival_time_origin) AS arrival_time_origin\n",
    "    FROM od_edges\n",
    "    GROUP BY\n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "        ,trip_id_origin\n",
    "        ,route_id_origin\n",
    "        ,direction_id_origin\n",
    "        \"\"\")\n",
    "\n",
    "# Preview\n",
    "no_duplicates_intermediate.createOrReplaceTempView(\"no_duplicates_intermediate\")\n",
    "spark.sql(\"SELECT * FROM no_duplicates_intermediate LIMIT 100\").show()\n",
    "del od_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have made sure we have only meaningful stop events, we further filter\n",
    "# We no longer care about specific trip/route IDs\n",
    "# Now we just want the distinct stop events that happen at each time (simultaneous service provides no additional value; no 0-minute headways)\n",
    "distinct_events = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "        ,arrival_time_origin\n",
    "    FROM no_duplicates_intermediate\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "distinct_events.createOrReplaceTempView(\"distinct_events\")\n",
    "spark.sql(\"SELECT * FROM distinct_events LIMIT 100\").show()\n",
    "del no_duplicates_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we count up the number of edges between each OD, post cleaning\n",
    "# We do this here instead of as a partition in the distinct_events table because distinct_events is still cleaning out dups we don't want counted\n",
    "od_instance_count = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "        ,COUNT(*) AS connection_count\n",
    "    FROM distinct_events\n",
    "    GROUP BY\n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "od_instance_count.createOrReplaceTempView(\"od_instance_count\")\n",
    "spark.sql(\"SELECT * FROM od_instance_count LIMIT 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_events_w_counts = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "         d.day_type_origin\n",
    "        ,d.buffer_center_origin\n",
    "        ,d.buffer_center_dest\n",
    "        ,d.hour_origin\n",
    "        ,d.arrival_time_origin\n",
    "        ,c.connection_count\n",
    "    FROM distinct_events d\n",
    "    LEFT JOIN od_instance_count c\n",
    "        ON d.day_type_origin = c.day_type_origin\n",
    "        AND d.buffer_center_origin = c.buffer_center_origin\n",
    "        AND d.buffer_center_dest = c.buffer_center_dest\n",
    "        AND d.hour_origin = c.hour_origin\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "distinct_events_w_counts.createOrReplaceTempView(\"distinct_events_w_counts\")\n",
    "spark.sql(\"SELECT * FROM distinct_events_w_counts LIMIT 100\").show()\n",
    "\n",
    "# Check that counts match\n",
    "spark.sql(\"SELECT COUNT(*) FROM distinct_events_w_counts\").show() #118,891,660\n",
    "spark.sql(\"SELECT COUNT(*) FROM distinct_events\").show() #118,891,660\n",
    "del distinct_events\n",
    "del od_instance_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to S3 for reference - then you can always just rerun the script from this point\n",
    "distinct_events_w_counts.write.csv(\"s3://massdot-test-bucket/checkpoint_v04_12_a_distinct_events_w_counts_2020_spring_recap.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to where there was only one connection\n",
    "single_connection_intermediate = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM distinct_events_w_counts\n",
    "    WHERE connection_count = 1\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "single_connection_intermediate.createOrReplaceTempView(\"single_connection_intermediate\")\n",
    "spark.sql(\"SELECT * FROM single_connection_intermediate LIMIT 100\").show()\n",
    "\n",
    "# Check how many single connection OD pairs there were on weekdays\n",
    "spark.sql(\"SELECT COUNT(*) FROM single_connection_intermediate WHERE day_type_origin = 'Weekday'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to S3 for reference - then you can always just rerun the script from this point\n",
    "single_connection_intermediate.write.csv(\"s3://massdot-test-bucket/checkpoint_v04_12_a_single_connection_intermediate_2020_spring_recap.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for where there were multiple connections, we get the lag for the headways\n",
    "all_events_lag = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        day_type_origin\n",
    "       ,buffer_center_origin\n",
    "       ,buffer_center_dest\n",
    "       ,hour_origin\n",
    "       ,arrival_time_origin\n",
    "       ,LAG(arrival_time_origin,1) OVER (\n",
    "          PARTITION BY day_type_origin, buffer_center_origin, buffer_center_dest, hour_origin\n",
    "          ORDER BY arrival_time_origin\n",
    "       ) previous_arrival\n",
    "    FROM distinct_events_w_counts\n",
    "    WHERE connection_count > 1\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "all_events_lag.createOrReplaceTempView(\"all_events_lag\")\n",
    "spark.sql(\"SELECT * FROM all_events_lag LIMIT 100\").show()\n",
    "#del distinct_events_w_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get the headway at each ODT where there remains more than one connection\n",
    "headways = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        day_type_origin\n",
    "       ,buffer_center_origin\n",
    "       ,buffer_center_dest\n",
    "       ,hour_origin\n",
    "       ,arrival_time_origin\n",
    "       ,previous_arrival\n",
    "       ,(arrival_time_origin - previous_arrival)/60 AS headway_minutes\n",
    "    FROM all_events_lag\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "headways.createOrReplaceTempView(\"headways\")\n",
    "spark.sql(\"SELECT * FROM headways LIMIT 100\").show()\n",
    "#del all_events_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headway_calculations_intermediate = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "        ,CAST(SUM(headway_minutes * headway_minutes) AS DOUBLE) / CAST(SUM(headway_minutes) AS DOUBLE) AS avg_expected_wait_time\n",
    "        ,COUNT(*) AS headway_count\n",
    "    FROM headways\n",
    "    WHERE headway_minutes IS NOT NULL\n",
    "    GROUP BY \n",
    "         day_type_origin\n",
    "        ,buffer_center_origin\n",
    "        ,buffer_center_dest\n",
    "        ,hour_origin\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "headway_calculations_intermediate.createOrReplaceTempView(\"headway_calculations_intermediate\")\n",
    "spark.sql(\"SELECT * FROM headway_calculations_intermediate LIMIT 100\").show()\n",
    "#del headways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to S3 for reference - then you can always just rerun the script from this point\n",
    "headway_calculations_intermediate.write.csv(\"s3://massdot-test-bucket/checkpoint_v4_12_a_headway_calculations_intermediate_2020_spring_recap.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have three reference tables calculated.  We need to put them in the proper format\n",
    "# This is the reference table for single-connection ODs: single_connection_intermediate\n",
    "# This is the reference table for ODs with more than one connection per hour: headway_calculations_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to edit the output tables we have to fit the format of the combined table\n",
    "# First let's take care of the single connections table\n",
    "single_connection_format = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         day_type_origin AS day_type\n",
    "        ,buffer_center_origin AS origin\n",
    "        ,buffer_center_dest AS destination\n",
    "        ,hour_origin AS hour\n",
    "        ,0 AS headway_count\n",
    "        ,45 AS expected_wait_time\n",
    "    FROM single_connection_intermediate\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "single_connection_format.createOrReplaceTempView(\"single_connection_format\")\n",
    "spark.sql(\"SELECT * FROM single_connection_format LIMIT 100\").show()\n",
    "#del single_connection_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the same as above for the other output table with headways\n",
    "headway_table_format = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "         day_type_origin AS day_type\n",
    "        ,buffer_center_origin AS origin\n",
    "        ,buffer_center_dest AS destination\n",
    "        ,hour_origin AS hour\n",
    "        ,headway_count\n",
    "        ,avg_expected_wait_time AS expected_wait_time\n",
    "    FROM headway_calculations_intermediate\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "headway_table_format.createOrReplaceTempView(\"headway_table_format\")\n",
    "spark.sql(\"SELECT * FROM headway_table_format LIMIT 100\").show()\n",
    "#del headway_calculations_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to bring these tables together into one\n",
    "od_frequency = spark.sql(\"\"\"\n",
    "    SELECT * FROM single_connection_format\n",
    "    UNION ALL\n",
    "    SELECT * FROM headway_table_format\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "od_frequency.createOrReplaceTempView(\"od_frequency\")\n",
    "spark.sql(\"SELECT * FROM od_frequency LIMIT 100\").show()\n",
    "#del single_connection_format\n",
    "#del headway_table_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to S3\n",
    "od_frequency.write.csv(\"s3://massdot-test-bucket/od_frequency_2020_spring_recap_20210413.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each OD has only one score\n",
    "test_no_dup = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "         origin\n",
    "        ,destination\n",
    "        ,hour\n",
    "        ,day_type\n",
    "        ,COUNT(*) AS count\n",
    "    FROM od_frequency\n",
    "    GROUP BY origin, destination, hour, day_type\n",
    "    \"\"\")\n",
    "\n",
    "# Preview\n",
    "test_no_dup.createOrReplaceTempView(\"test_no_dup\")\n",
    "spark.sql(\"SELECT * FROM test_no_dup WHERE count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many records are in the final table\n",
    "# SELECT COUNT(*) FROM jwoltal.od_frequency_spring_2020_v7 --45,378,131\n",
    "spark.sql(\"SELECT COUNT(*) FROM od_frequency\").show() # 45,378,131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample OD\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM od_frequency\n",
    "    WHERE\n",
    "        origin = '81852'\n",
    "        AND destination IN ('91852', 'place-NB-0120', 'Needham Junction')\n",
    "        AND hour = 8\n",
    "        AND day_type = 'Weekday'\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample OD\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM od_frequency\n",
    "    WHERE\n",
    "        origin = 'place-coecl'\n",
    "        AND destination = 'place-nuniv'\n",
    "        AND hour = 8\n",
    "        AND day_type = 'Weekday'\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also spot check the old version of frequency which was calculated in the research server - do they look like they match?\n",
    "df = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/frequency\")\n",
    "\n",
    "df.createOrReplaceTempView(\"old_freq_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample OD in the old data\n",
    "# It's a match.  So was the overall record count.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM old_freq_table\n",
    "    WHERE\n",
    "        origin = 'place-coecl'\n",
    "        AND destination = 'place-nuniv'\n",
    "        AND hour = 8\n",
    "        AND day_type = 'Weekday'\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join everything together and find the difference between the values\n",
    "diff = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "         a.day_type\n",
    "        ,a.origin\n",
    "        ,a.destination\n",
    "        ,a.hour\n",
    "        ,a.headway_count AS headway_count_new\n",
    "        ,a.expected_wait_time AS expected_wait_new\n",
    "        ,b.headway_count AS headway_count_old\n",
    "        ,b.expected_wait_time AS expected_wait_old\n",
    "        ,ABS(b.expected_wait_time - a.expected_wait_time) AS delta\n",
    "    FROM od_frequency a \n",
    "    LEFT JOIN old_freq_table b \n",
    "        ON a.day_type = b.day_type\n",
    "        AND a.origin = b.origin\n",
    "        AND a.destination = b.destination\n",
    "        AND a.hour = b.hour\n",
    "    \"\"\")\n",
    "    \n",
    "diff.createOrReplaceTempView(\"diff\")\n",
    "spark.sql(\"SELECT * FROM diff\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the difference is not just a rounding error based on different versions of SQL\n",
    "# Note that the output calculated here is very infrequently different from that calculated using v6 in the research server (which is what was loaded onto S3)\n",
    "# I investigated the cause of these edge cases\n",
    "# v6 did not properly fix the parent-child combinations more than 100m apart issue.  This was fixed in v7, so the dataframes should match there\n",
    "# The values calculated here take this into account properly.  I have also confirmed an example using the raw GTFS schedule\n",
    "# A file testing this is saved in \"C:\\Users\\woltalj\\Box\\Ongoing Activities\\SQL Queries\\Service Delivery Policy 2020\\Frequency\\OD Frequency\\OD Frequency S3\\S3-MGHPCC Diff Check.xlsx\"\n",
    "spark.sql(\"SELECT COUNT(*) FROM diff WHERE delta > .1\").show()\n",
    "spark.sql(\"SELECT * FROM diff WHERE delta > .1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = spark.sql(\"SELECT * FROM diff WHERE delta > .1\")\n",
    "error.write.csv(\"s3://massdot-test-bucket/method_delta.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## How to pick up at a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint data from S3\n",
    "\n",
    "# There are three available checkpoints:\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_distinct_events_w_counts_2020_spring_recap.csv\"\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_headway_calculations_intermediate_2020_spring_recap.csv\"\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_single_connection_intermediate_2020_spring_recap.csv\"\n",
    "df = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/checkpoint_v2_headway_calculations_intermediate_2020_spring_recap.csv\")\n",
    "\n",
    "# Save it under a table the script recognizes, then continue running above\n",
    "# You can save it as:\n",
    "## distinct_events_w_counts\n",
    "## headway_calculations_intermediate\n",
    "## single_connection_intermediate\n",
    "df.createOrReplaceTempView(\"headway_calculations_intermediate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM headway_calculations_intermediate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint data from S3\n",
    "\n",
    "# There are two available checkpoints:\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_distinct_events_w_counts_2020_spring_recap.csv\"\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_headway_calculations_intermediate_2020_spring_recap.csv\"\n",
    "df = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/checkpoint_v04_12_a_distinct_events_w_counts_2020_spring_recap.csv\")\n",
    "\n",
    "# Save it under a table the script recognizes, then continue running above\n",
    "# You can save it as:\n",
    "## distinct_events_w_counts\n",
    "## headway_calculations_intermediate\n",
    "df.createOrReplaceTempView(\"distinct_events_w_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM distinct_events_w_counts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint data from S3\n",
    "\n",
    "# There are two available checkpoints:\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_distinct_events_w_counts_2020_spring_recap.csv\"\n",
    "## \"s3://massdot-test-bucket/checkpoint_v2_headway_calculations_intermediate_2020_spring_recap.csv\"\n",
    "df = spark\\\n",
    "    .read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3://massdot-test-bucket/checkpoint_v04_12_a_single_connection_intermediate_2020_spring_recap.csv\")\n",
    "\n",
    "# Save it under a table the script recognizes, then continue running above\n",
    "# You can save it as:\n",
    "## distinct_events_w_counts\n",
    "## headway_calculations_intermediate\n",
    "df.createOrReplaceTempView(\"single_connection_intermediate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM single_connection_intermediate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample OD\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM distinct_events_w_counts\n",
    "    WHERE\n",
    "        buffer_center_origin = '81852'\n",
    "        AND buffer_center_dest IN ('91852', 'place-NB-0120', 'Needham Junction')\n",
    "        AND hour_origin = 8\n",
    "        AND day_type_origin = 'Weekday'\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
